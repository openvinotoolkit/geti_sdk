# Copyright (C) 2024 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions
# and limitations under the License.

import logging
import os
import tempfile
from typing import List, Union

import cv2
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from tqdm.auto import tqdm

from geti_sdk import Geti
from geti_sdk.data_models import Prediction, Project
from geti_sdk.data_models.enums.task_type import TaskType
from geti_sdk.data_models.project import Dataset
from geti_sdk.deployment import Deployment
from geti_sdk.rest_clients import AnnotationClient, ImageClient, ModelClient

from .ood_data import DistributionDataItem, DistributionDataItemPurpose
from .ood_sub_model import KNNBasedOODModel, ProbabilityBasedModel
from .utils import (
    CutoutTransform,
    calculate_ood_metrics,
    get_deployment_with_xai_head,
    image_to_distribution_data_item,
    load_annotations,
    ood_metrics_to_string,
    split_data,
)

# Names of the Geti datasets that are used as reference/training data for the COOD model, if available
ID_DATASET_NAMES = ["Dataset"]
OOD_DATASET_NAMES = ["OOD reference dataset"]


class COODModel:
    """
    Out-of-distribution detection model. Uses the Combined out-of-distribution (COOD) detection
    algorithm (see : https://arxiv.org/abs/2403.06874).
    """

    def __init__(
        self,
        geti: Geti,
        project: Union[str, Project],
        deployment: Deployment = None,
        ood_images_dir: str = None,
        workspace_dir: str = None,
    ):
        """
        Model for Combined Out-of-Distribution (COOD) detection .
        :param geti: Geti instance representing the GETi server from which the project is to be used.
        :param project: Project or project name to use to fetch the deployment and the in-distribution data.
        The project must exist on the specified Geti instance and should have at least one trained model.
        :param deployment: Deployment to use for learning the data distribution. If None, a deployment with an XAI head is
        automatically selected from the project. If this COODModel is used in an OODTrigger,then make sure that
        the same deployment is used for post-inference hook.
        :param ood_images_dir: Path to the directory containing out-of-distribution images for training. If
        not provided, near-OOD images are generated by applying strong corruptions to the in-distribution images.
        """
        self.geti = geti

        self.id_distribution_data = List[DistributionDataItem]
        self.ood_distribution_data = List[DistributionDataItem]
        self.ood_reference_images_dir = ood_images_dir

        self.ood_classifier = None  # The COOD random forest classifier

        # Initial values for the best thresholds for the COOD model.
        # Once the model is trained, these are updated based on a small validation set
        self.best_thresholds = {
            "fscore": 0.5,
            "tpr_at_1_fpr": 0.5,
            "tpr_at_5_fpr": 0.5,
        }
        self._thresholds_prefix = "threshold_"
        self.train_test_split_ratio = 0.7  # The ratio of train-test split
        self.eval_metrics = {}

        if isinstance(project, str):
            project_name = project
            self.project = geti.get_project(project_name=project_name)
        else:
            self.project = project

        self.model_client = ModelClient(
            session=self.geti.session,
            workspace_id=self.geti.workspace_id,
            project=self.project,
        )
        self.image_client = ImageClient(
            session=self.geti.session,
            workspace_id=self.geti.workspace_id,
            project=self.project,
        )

        self.annotation_client = AnnotationClient(
            session=self.geti.session,
            workspace_id=self.geti.workspace_id,
            project=self.project,
        )

        if workspace_dir is None:
            workspace_dir = tempfile.mkdtemp()

        self.workspace_dir = os.path.join(
            workspace_dir, "ood_detection", self.project.name
        )

        os.makedirs(self.workspace_dir, exist_ok=True)

        self.data_dir = os.path.join(self.workspace_dir, "data")

        # Checks if project is a single-task classification project
        self._check_project_fit()

        if deployment is None:
            # If no deployment is provided, select an XAI model with the highest accuracy to be deployed
            self.deployment = get_deployment_with_xai_head(
                geti=self.geti, model_client=self.model_client
            )
        else:
            if not deployment.models[0].has_xai_head:
                raise ValueError(
                    "The provided deployment does not have an model with an XAI head."
                    "Please reconfigure the deployment to include a model with an XAI head "
                    "(OptimizedModel.has_xai_head must be True). "
                    "Hint : You can use the get_deployment_with_xai_head() method from detect_ood.utils"
                )

            self.deployment = deployment

        if not self.deployment.are_models_loaded:
            self.deployment.load_inference_models(device="CPU")

        logging.info(
            f"Building Combined OOD detection model for Intel® Geti™ project `{self.project.name}`."
        )

        # The transformation to apply to in-distribution images to generate near-OOD images
        # if no OOD images are provided
        self.corruption_transform = (
            CutoutTransform() if self.ood_reference_images_dir is None else None
        )

        # Download, extract, and prepare the in-distribution and out-of-distribution data
        distribution_data = self._prepare_id_ood_data()
        self.id_distribution_data = distribution_data["id_data"]
        self.ood_distribution_data = distribution_data["ood_data"]

        # Split the data into train and test sets. The COOD model once trained (on the train split) will be evaluated on
        # the test split. This accuracy is indicated to the user and if a user feels the accuracy is too low,
        # they can create a new COOD model with more/better data.

        train_id_data, test_id_data = split_data(
            data=self.id_distribution_data,
            stratified=True,
            split_ratio=self.train_test_split_ratio,
        )
        train_ood_data, test_ood_data = split_data(
            data=self.ood_distribution_data,
            stratified=False,
            split_ratio=self.train_test_split_ratio,
        )

        # A dict consisting of smaller OOD models (FRE, EnWeDi, etc)
        self.sub_models = {
            "knn_based": KNNBasedOODModel(knn_k=10),
            # "class_fre": ClassFREBasedModel(n_components=0.90),
            # "global_fre": GlobalFREBasedModel(n_components=0.90),
            "max_softmax_probability": ProbabilityBasedModel(),
        }

        self._train_cood_model(id_data=train_id_data, ood_data=train_ood_data)
        logging.info("COOD Model is trained and ready for inference.")

        eval_metrics_test = self._test_model(
            test_id_data=test_id_data, test_ood_data=test_ood_data
        )
        self.eval_metrics["test"] = eval_metrics_test
        logging.info(
            f"COOD Model Metrics on Test Data: {ood_metrics_to_string(eval_metrics_test)}"
        )

    def _prepare_id_ood_data(self) -> dict:
        """
        Prepare the in-distribution and out-of-distribution data for training the COOD and the sub-models.
        For in-distribution data, all the images and annotations from the dataset names mentioned in ID_DATASET_NAMES
        are downloaded from the Geti Project.
        For out-of-distribution data, the same is done for the dataset names mentioned in OOD_DATASET_NAMES. If there is
        no out-of-distribution dataset, near-OOD images are generated by applying strong corruptions to the downloaded
        in-distribution images.
        All images are locally inferred to get prediction probabilities and feature vectors
        """
        datasets_in_project = self.project.datasets

        id_datasets_in_geti = []
        ood_datasets_in_geti = []

        # Figure out the datasets that can be used as a reference for in-distribution and out-of-distribution data
        for dataset in datasets_in_project:
            if dataset.name in ID_DATASET_NAMES:
                id_datasets_in_geti.append(dataset)
            elif dataset.name in OOD_DATASET_NAMES:
                ood_datasets_in_geti.append(dataset)

        if len(id_datasets_in_geti) == 0:
            raise ValueError(
                "Could not find any relevant datasets for in-distribution data. "
                "Please make sure that the project contains at least one dataset with the names: "
                f"{ID_DATASET_NAMES}."
            )

        id_distribution_data_items = []  # List[DistributionDataItem]
        for dataset in id_datasets_in_geti:
            logging.info(f"Extracting ID data from the Geti dataset '{dataset.name}'")
            id_distribution_data_items.extend(
                self._prepare_distribution_data(source=dataset)
            )

        # For OOD data, this is the order of preference :
        # 1) Use the reference images provided by the user
        # 2) If not provided, use the OOD datasets in the project
        # 3) If no OOD datasets are found, generate near-OOD images by applying strong
        # corruptions to the in-distribution images

        ood_distribution_data_items = []  # List[DistributionDataItem]
        if self.ood_reference_images_dir:
            # Case 1: Use the reference images provided by the user
            logging.info(
                f"Extracting OOD data from the provided reference images in {self.ood_reference_images_dir}"
            )

            ood_distribution_data_items = self._prepare_distribution_data(
                source=self.ood_reference_images_dir
            )

        else:
            if len(ood_datasets_in_geti) == 0:
                # Case 3: If no OOD datasets are found, generate near-OOD images by applying strong corruptions
                logging.info(
                    "No out-of-distribution datasets found in the project. "
                    "Generating near-OOD images by applying strong corruptions to the in-distribution images."
                )
                for dataset in id_datasets_in_geti:
                    ood_path = self._create_ood_images(reference_dataset=dataset)
                    ood_distribution_data_items.extend(
                        self._prepare_distribution_data(source=ood_path)
                    )

            else:
                # Case 2: Use the OOD datasets in the project
                for dataset in ood_datasets_in_geti:
                    logging.info(
                        f"Extracting OOD data from the Geti dataset '{dataset.name}'"
                    )
                    ood_distribution_data_items.extend(
                        self._prepare_distribution_data(source=dataset)
                    )

        logging.info(
            f"Number of in-distribution samples: {len(id_distribution_data_items)}"
        )
        logging.info(
            f"Number of out-of-distribution samples: {len(ood_distribution_data_items)}"
        )

        return {
            "id_data": id_distribution_data_items,
            "ood_data": ood_distribution_data_items,
        }

    def _train_sub_models(self, train_data: List[DistributionDataItem]) -> None:
        """
        Train the sub-models for OOD detection.
        Currently, all sub-models are trained on in-distribution data.
        """
        for sub_model in self.sub_models.values():
            sub_model.train(distribution_data=train_data)

    def _train_cood_model(
        self,
        id_data: List[DistributionDataItem],
        ood_data: List[DistributionDataItem],
    ) -> None:
        """
        Train the COOD model using the RandomForestClassifier
        :param id_data: List of DistributionDataItems for in-distribution data (Train split)
        :param ood_data: List of DistributionDataItems for out-of-distribution data (Train split)
        """
        logging.info("Training COOD Model")
        logging.info(f"Training data: ID - {len(id_data)}, OOD - {len(ood_data)}")

        id_data_train, id_data_val = split_data(
            data=id_data,
            stratified=True,
            split_ratio=self.train_test_split_ratio,
            purposes=(
                DistributionDataItemPurpose.TRAIN,
                DistributionDataItemPurpose.VAL,
            ),
        )
        ood_data_train, ood_data_val = split_data(
            data=ood_data,
            stratified=False,
            split_ratio=self.train_test_split_ratio,
            purposes=(
                DistributionDataItemPurpose.TRAIN,
                DistributionDataItemPurpose.VAL,
            ),
        )

        self._train_sub_models(train_data=id_data_train)

        id_train_cood_features = self._cood_features_from_distribution_data(
            distribution_data=id_data_train
        )
        ood_train_cood_features = self._cood_features_from_distribution_data(
            distribution_data=ood_data_train
        )

        id_val_cood_features = self._cood_features_from_distribution_data(
            distribution_data=id_data_val
        )
        ood_val_cood_features = self._cood_features_from_distribution_data(
            distribution_data=ood_data_val
        )

        all_features_train = np.concatenate(
            (id_train_cood_features, ood_train_cood_features)
        )
        # We take ood images as True or 1 and id images as False or 0
        all_labels_train = np.concatenate(
            (
                np.zeros(len(id_train_cood_features)),
                np.ones(len(ood_train_cood_features)),
            )
        )

        all_features_val = np.concatenate((id_val_cood_features, ood_val_cood_features))
        all_labels_val = np.concatenate(
            (
                np.zeros(len(id_val_cood_features)),
                np.ones(len(ood_val_cood_features)),
            )
        )

        # self._train_cood_hpo(
        #     id_features_train=id_train_cood_features,
        #     id_features_val=id_val_cood_features,
        #     ood_features_train=ood_train_cood_features,
        #     ood_features_val=ood_val_cood_features,
        # )

        # Train the RandomForestClassifier
        self.ood_classifier = RandomForestClassifier(random_state=42)
        self.ood_classifier.fit(all_features_train, all_labels_train)

        # Evaluate the model on the train and validation data
        # Calculate the probabilities for the train and validation data
        # We take the probability of the image being OOD only
        pred_probabilities_train = self.ood_classifier.predict_proba(
            all_features_train
        )[:, 1]
        pred_probabilities_val = self.ood_classifier.predict_proba(all_features_val)[
            :, 1
        ]

        eval_metrics_train = calculate_ood_metrics(
            y_true=all_labels_train, y_pred_prob=pred_probabilities_train
        )

        eval_metrics_val = calculate_ood_metrics(
            y_true=all_labels_val,
            y_pred_prob=pred_probabilities_val,
        )

        logging.info(
            f"COOD Model Metrics on Training Data: \n {ood_metrics_to_string(eval_metrics_train)}"
        )

        logging.info(
            f"COOD Model Metrics on Validation Data: \n {ood_metrics_to_string(eval_metrics_val)}"
        )

        self.eval_metrics["train"] = eval_metrics_train
        self.eval_metrics["val"] = eval_metrics_val

        # Update the prediction thresholds based on validation data
        self._update_thresholds(eval_metrics=eval_metrics_val)

    def _update_thresholds(self, eval_metrics: dict) -> None:
        """
        Update the best thresholds for the COOD model based on the evaluation metrics
        :param eval_metrics: A dictionary containing the evaluation metrics for the COOD model.
        """
        for threshold_name in self.best_thresholds:
            if (
                self._thresholds_prefix + threshold_name
            ) in eval_metrics and eval_metrics[
                self._thresholds_prefix + threshold_name
            ] is not None:
                self.best_thresholds[threshold_name] = eval_metrics[
                    self._thresholds_prefix + threshold_name
                ]

    def _cood_features_from_distribution_data(
        self, distribution_data: List[DistributionDataItem]
    ) -> np.ndarray:
        """
        Extract the COOD features from the distribution data.
        :param distribution_data: List of DistributionDataItems for which the COOD features are extracted.
        :return: Numpy array of COOD features
        """
        scores_all_sub_models = self._infer_sub_models(distribution_data)
        features_arranged = self._aggregate_sub_model_scores_into_cood_features(
            scores_all_sub_models=scores_all_sub_models,
            num_images=len(distribution_data),
        )
        return features_arranged

    def _infer_sub_models(self, distribution_data: List[DistributionDataItem]) -> dict:
        """
        Get OOD scores from all the sub-models for the given distribution data.
        :param distribution_data: List of DistributionDataItems for which ood scores are calculated
        :return: A dictionary containing the OOD scores from all the sub-models.
        """
        scores_all_sub_models = {}
        for sub_model in self.sub_models.values():
            scores_dict = sub_model(distribution_data)
            for score_type in scores_dict:
                scores_all_sub_models[score_type] = scores_dict[score_type]
        return scores_all_sub_models

    def _test_model(
        self,
        test_id_data: List[DistributionDataItem],
        test_ood_data: List[DistributionDataItem],
    ) -> dict:
        """
        Evaluate the COOD model on the test data.
        :param test_id_data: List of DistributionDataItems for in-distribution test data
        :param test_ood_data: List of DistributionDataItems for out-of-distribution test data
        :return: A dictionary containing the evaluation metrics for the COOD model.
        """
        # Note that a simpler way to evaluate the model is to call the self.__call__ method with the prediction of
        # each data item. However, this might be slower when the test set is large as we need to call the __call__
        # method for each data item.

        num_id_images = len(test_id_data)
        num_ood_images = len(test_ood_data)

        logging.info("Evaluating COOD Model on Test Data")
        logging.info(f"Test data: ID - {num_id_images}, OOD - {num_ood_images}")

        # Get scores from sub-models
        id_scores_all_sub_models = self._infer_sub_models(test_id_data)
        ood_scores_all_sub_models = self._infer_sub_models(test_ood_data)

        # Arrange features
        id_features = self._aggregate_sub_model_scores_into_cood_features(
            id_scores_all_sub_models, num_id_images
        )
        ood_features = self._aggregate_sub_model_scores_into_cood_features(
            ood_scores_all_sub_models, num_ood_images
        )

        # Combine features and labels
        all_features = np.concatenate((id_features, ood_features))
        all_labels = np.concatenate((np.zeros(num_id_images), np.ones(num_ood_images)))
        ood_probability = self.ood_classifier.predict_proba(all_features)[:, 1]

        eval_metrics = calculate_ood_metrics(
            y_true=all_labels, y_pred_prob=ood_probability
        )

        return eval_metrics

    def __call__(self, prediction: Prediction) -> float:
        """
        Return the COOD Score based using feature vector and prediction probabilities in "prediction".
        """
        data_item = DistributionDataItem(
            media_name="sample",  # We do not need this data for inference
            media_path="sample",
            annotated_label="",
            raw_prediction=prediction,
        )
        scores_all_sub_models = self._infer_sub_models(distribution_data=[data_item])
        features_arranged = self._aggregate_sub_model_scores_into_cood_features(
            scores_all_sub_models=scores_all_sub_models, num_images=1
        )

        cood_score = self.ood_classifier.predict_proba(features_arranged)

        return cood_score[0][1]  # Return only the probability of being OOD

    def _create_ood_images(self, reference_dataset: Dataset) -> str:
        """
        Create near-OOD images by applying strong corruptions to the images in the reference datasets.
        """
        # Options  : Applying corruptions, generating Perlin Noise Images, Background extraction (using saliency maps)
        ref_images_path = os.path.join(self.data_dir, reference_dataset.name, "images")
        corrupted_images_path = os.path.join(self.data_dir, "ood_images")
        if not os.path.exists(corrupted_images_path):
            os.makedirs(corrupted_images_path)

        for image_name in tqdm(
            os.listdir(ref_images_path), desc="Generating OOD images"
        ):
            image_path = os.path.join(ref_images_path, image_name)
            img = cv2.imread(image_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            corrupted_img = self.corruption_transform(img)
            corrupted_image_path = os.path.join(corrupted_images_path, image_name)
            cv2.imwrite(corrupted_image_path, corrupted_img)

        return corrupted_images_path

    def _check_project_fit(self):
        """
        Check if the project is suited for the current OOD detection task.
        Currently, only a single task of type "classification" is supported.
        """
        tasks_in_project = self.project.get_trainable_tasks()
        if len(tasks_in_project) != 1:
            raise ValueError(
                "Out-of-distribution detection is only "
                "supported for projects with a single task for now."
            )

        # get the task type and check if it is classification
        task_type = tasks_in_project[0].task_type
        if task_type != TaskType.CLASSIFICATION:
            raise ValueError(
                "Out-of-distribution detection models are only "
                "supported for classification tasks for now."
            )

    def _prepare_distribution_data(
        self, source: Union[Dataset, str]
    ) -> List[DistributionDataItem]:
        """
        Prepare the distribution data from the source by inferencing the images and extracting the feature vectors.
        param source: Dataset or directory containing images. If a dataset is provided, the images and annotations are
        downloaded from the dataset. If a directory is provided, the images are read from the directory.
        """
        valid_extensions = (".jpg", ".jpeg", ".png", ".bmp", ".gif", ".tiff")

        if isinstance(source, Dataset):

            media_list = self.image_client.get_all_images(dataset=source)
            self.image_client.download_all(
                path_to_folder=self.data_dir, append_image_uid=True, dataset=source
            )

            self.annotation_client.download_annotations_for_images(
                images=media_list,
                path_to_folder=self.data_dir,
                append_image_uid=True,
            )

            annotations_dir = os.path.join(self.data_dir, "annotations")
            image_dir = os.path.join(self.data_dir, "images", source.name)

            image_paths = [
                os.path.join(image_dir, f"{media.name}_{media.id}.jpg")
                for media in media_list
            ]
            annotation_files = [
                os.path.join(annotations_dir, f"{media.name}_{media.id}.json")
                for media in media_list
            ]
        else:
            # Find all the images in the directory
            image_paths = [
                os.path.join(source, image_name)
                for image_name in os.listdir(source)
                if image_name.lower().endswith(valid_extensions)
            ]

            annotation_files = [None] * len(image_paths)

        distribution_data_items = []
        for image_path, annotation_file in zip(image_paths, annotation_files):
            annotation_label = (
                load_annotations(annotation_file) if annotation_file else None
            )
            data_item = image_to_distribution_data_item(
                deployment=self.deployment,
                image_path=image_path,
                annotation_label=annotation_label,
            )
            distribution_data_items.append(data_item)

        return distribution_data_items

    @staticmethod
    def _aggregate_sub_model_scores_into_cood_features(
        scores_all_sub_models: dict, num_images: int
    ) -> np.ndarray:
        """
        Combine the OOD scores from all the sub-models into a single feature vector that can be passed to the
        COOD's random forest classifier
        :param scores_all_sub_models: A dictionary containing the OOD scores from all the sub-models
        :param num_images: Number of images for which the OOD scores are calculated
        :return: A feature vector containing the OOD scores from all the sub-models as a numpy array
        """
        features = np.zeros((num_images, len(scores_all_sub_models)))

        for score_idx, score_type in enumerate(scores_all_sub_models):
            features[:, score_idx] = scores_all_sub_models[score_type]
        return features

    def __repr__(self):
        """
        Return a string representation of the COODModel.
        """
        return (
            f"COODModel(project={self.project.name}, "
            f"Sub models: {list(self.sub_models.keys())}, "
            f"Data Items: {len(self.id_distribution_data)} ID, {len(self.ood_distribution_data)} OOD)"
        )
